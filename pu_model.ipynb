{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request, xmltodict, feedparser\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "import pprint\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_abs_arxiv(url):\n",
    "    if 'arxiv' not in url: return -1,{}\n",
    "    if 'pdf' in url: arxiv_id=''.join(url.split('pdf')[-2:-1])[1:-1]\n",
    "    else: arxiv_id=url.split('/')[-1]\n",
    "    query_url='http://export.arxiv.org/api/query?id_list='+arxiv_id\n",
    "    data_dict = xmltodict.parse(urllib.request.urlopen(query_url).read())['feed']\n",
    "    return 0,{'title':data_dict['entry']['title'],'abstract':' '.join(data_dict['entry']['summary'].split('\\n'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_daily_arxiv_papers(cat='cs.LG'):\n",
    "    feedurl=f'https://rss.arxiv.org/rss/{cat}'\n",
    "    feed=feedparser.parse(feedurl)\n",
    "    datestr=datetime.fromtimestamp(mktime(feed['feed']['published_parsed'])).strftime('%d-%b-%Y')\n",
    "    daily_papers=[]\n",
    "    for e in feed['entries']:\n",
    "        paper_entry={'id':e['id'],'title':e['title'],\n",
    "                     'abstract':' '.join(e['summary'].split('\\n')),\n",
    "                     'date':datestr}\n",
    "        daily_papers.append(paper_entry)\n",
    "    return daily_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_urls = pd.read_csv('papers_of_interest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://arxiv.org/pdf/2209.11142.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://arxiv.org/pdf/2212.02475.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://arxiv.org/pdf/2211.16564.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://arxiv.org/abs/1803.05316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://arxiv.org/abs/2206.08896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>https://arxiv.org/pdf/2401.03006.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>310</td>\n",
       "      <td>https://arxiv.org/pdf/2403.04732.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>311</td>\n",
       "      <td>https://arxiv.org/pdf/2404.08819.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>312</td>\n",
       "      <td>https://arxiv.org/pdf/2404.11483v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>313</td>\n",
       "      <td>https://arxiv.org/pdf/2404.06921.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      no                                     url\n",
       "0      1    https://arxiv.org/pdf/2209.11142.pdf\n",
       "1      2    https://arxiv.org/pdf/2212.02475.pdf\n",
       "2      3    https://arxiv.org/pdf/2211.16564.pdf\n",
       "3      4        https://arxiv.org/abs/1803.05316\n",
       "4      5        https://arxiv.org/abs/2206.08896\n",
       "..   ...                                     ...\n",
       "308  309    https://arxiv.org/pdf/2401.03006.pdf\n",
       "309  310    https://arxiv.org/pdf/2403.04732.pdf\n",
       "310  311    https://arxiv.org/pdf/2404.08819.pdf\n",
       "311  312  https://arxiv.org/pdf/2404.11483v1.pdf\n",
       "312  313    https://arxiv.org/pdf/2404.06921.pdf\n",
       "\n",
       "[313 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_content = []\n",
    "# for url in train_urls['url']:\n",
    "#     status, content = get_title_abs_arxiv(url)\n",
    "#     # print(content)\n",
    "#     # break\n",
    "#     if status == 0:\n",
    "#         train_content.append(content)\n",
    "#     else:\n",
    "#         print(f\"Failed to fetch content for {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train_content to a pandas dataframe\n",
    "df = pd.read_csv('train_content.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('train_content.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "loading file vocab.txt from cache at C:\\Users\\Gautam\\.cache\\huggingface\\hub\\models--allenai--specter2_base\\snapshots\\88fcaeab633f2cadae64ee13aa822db1917d5a05\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Gautam\\.cache\\huggingface\\hub\\models--allenai--specter2_base\\snapshots\\88fcaeab633f2cadae64ee13aa822db1917d5a05\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\Gautam\\.cache\\huggingface\\hub\\models--allenai--specter2_base\\snapshots\\88fcaeab633f2cadae64ee13aa822db1917d5a05\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Gautam\\.cache\\huggingface\\hub\\models--allenai--specter2_base\\snapshots\\88fcaeab633f2cadae64ee13aa822db1917d5a05\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Gautam\\.cache\\huggingface\\hub\\models--allenai--specter2_base\\snapshots\\88fcaeab633f2cadae64ee13aa822db1917d5a05\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/specter2_base\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Gautam\\.cache\\huggingface\\hub\\models--allenai--specter2_base\\snapshots\\88fcaeab633f2cadae64ee13aa822db1917d5a05\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at allenai/specter2_base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "model = AutoModel.from_pretrained('allenai/specter2_base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'abstract'],\n",
       "    num_rows: 313\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0]['title'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((tokenizer(dataset['title'][0]))['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=[]\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    input_string = dataset['title'][i] + ' ' + dataset['abstract'][i]\n",
    "    tokenized_input = tokenizer(input_string, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "    tokenized_input = {k: v.cuda() for k, v in tokenized_input.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # embedding = outputs.last_hidden_state[:,1,-1,:].squeeze(0)\n",
    "        # print(last_hidden_states.shape)\n",
    "        # print(pooled_output.shape)\n",
    "        embeddings.append(pooled_output.cpu())\n",
    "        # print(pooled_output)\n",
    "        # print(embedding.shape)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Stack the tensors along a new dimension\n",
    "stacked_embeddings = torch.stack(embeddings)\n",
    "\n",
    "# Compute the mean along that dimension\n",
    "mean_embeddings = torch.mean(stacked_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       arXiv:2403.19669v1 Announce Type: new  Abstrac...\n",
       "1       arXiv:2403.19717v1 Announce Type: new  Abstrac...\n",
       "2       arXiv:2403.19721v1 Announce Type: new  Abstrac...\n",
       "3       arXiv:2403.19792v1 Announce Type: new  Abstrac...\n",
       "4       arXiv:2403.19800v1 Announce Type: new  Abstrac...\n",
       "                              ...                        \n",
       "1099    The advent of large language models (LLMs) has...\n",
       "1100    In this paper, we analyze the impact of data f...\n",
       "1101    Complex single-objective bounded problems are ...\n",
       "1102    Given a causal graph representing the data-gen...\n",
       "1103    In this paper, we present OmniSearchSage, a ve...\n",
       "Name: abstract, Length: 1104, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_data = pd.read_csv('daily_arxiv_papers.csv') \n",
    "# remove the rows with id , title , abstract, date as a row\n",
    "\n",
    "ids = []\n",
    "for i in range(len(unlabelled_data)):\n",
    "    if unlabelled_data['id'][i] == 'id':\n",
    "        ids.append(i)\n",
    "\n",
    "\n",
    "# remove the rows with id , title , abstract, date as a row\n",
    "unlabelled_data = unlabelled_data.drop(ids)\n",
    "\n",
    "# make the indices start from 0\n",
    "unlabelled_data = unlabelled_data.reset_index(drop=True)\n",
    "\n",
    "# remove arXiv:2403.19669v1 Announce Type: new  Abstract: from the abstract\n",
    "abstracts = unlabelled_data['abstract']\n",
    "abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_abstracts = []\n",
    "for abstract in abstracts:\n",
    "    abstract = abstract.split('Abstract: ')\n",
    "    if len(abstract) > 1:\n",
    "        abstract = abstract[1]\n",
    "    else:\n",
    "        abstract = abstract[0]\n",
    "    new_abstracts.append(abstract)\n",
    "# new_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new abstracts is new absracts for unlabeled data\n",
    "unlabelled_data['abstract'] = new_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_data.drop_duplicates(subset=['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oai:arXiv.org:2403.19669v1</td>\n",
       "      <td>Analyzing the Roles of Language and Vision in ...</td>\n",
       "      <td>Does language help make sense of the visual wo...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oai:arXiv.org:2403.19717v1</td>\n",
       "      <td>A Picture is Worth 500 Labels: A Case Study of...</td>\n",
       "      <td>Mobile apps have embraced user privacy by movi...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oai:arXiv.org:2403.19721v1</td>\n",
       "      <td>Computationally and Memory-Efficient Robust Pr...</td>\n",
       "      <td>In the current data-intensive era, big data ha...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oai:arXiv.org:2403.19792v1</td>\n",
       "      <td>MAPL: Model Agnostic Peer-to-peer Learning</td>\n",
       "      <td>Effective collaboration among heterogeneous cl...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oai:arXiv.org:2403.19800v1</td>\n",
       "      <td>Gegenbauer Graph Neural Networks for Time-vary...</td>\n",
       "      <td>Reconstructing time-varying graph signals (or ...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>http://arxiv.org/abs/2404.16283v1</td>\n",
       "      <td>Andes: Defining and Enhancing Quality-of-Exper...</td>\n",
       "      <td>The advent of large language models (LLMs) has...</td>\n",
       "      <td>2024-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>http://arxiv.org/abs/2404.16281v1</td>\n",
       "      <td>Timely Communications for Remote Inference</td>\n",
       "      <td>In this paper, we analyze the impact of data f...</td>\n",
       "      <td>2024-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>http://arxiv.org/abs/2404.16280v1</td>\n",
       "      <td>An Efficient Reconstructed Differential Evolut...</td>\n",
       "      <td>Complex single-objective bounded problems are ...</td>\n",
       "      <td>2024-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>http://arxiv.org/abs/2404.16277v1</td>\n",
       "      <td>Causally Inspired Regularization Enables Domai...</td>\n",
       "      <td>Given a causal graph representing the data-gen...</td>\n",
       "      <td>2024-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>http://arxiv.org/abs/2404.16260v1</td>\n",
       "      <td>OmniSearchSage: Multi-Task Multi-Entity Embedd...</td>\n",
       "      <td>In this paper, we present OmniSearchSage, a ve...</td>\n",
       "      <td>2024-04-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>836 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0           oai:arXiv.org:2403.19669v1   \n",
       "1           oai:arXiv.org:2403.19717v1   \n",
       "2           oai:arXiv.org:2403.19721v1   \n",
       "3           oai:arXiv.org:2403.19792v1   \n",
       "4           oai:arXiv.org:2403.19800v1   \n",
       "..                                 ...   \n",
       "981  http://arxiv.org/abs/2404.16283v1   \n",
       "982  http://arxiv.org/abs/2404.16281v1   \n",
       "983  http://arxiv.org/abs/2404.16280v1   \n",
       "984  http://arxiv.org/abs/2404.16277v1   \n",
       "985  http://arxiv.org/abs/2404.16260v1   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Analyzing the Roles of Language and Vision in ...   \n",
       "1    A Picture is Worth 500 Labels: A Case Study of...   \n",
       "2    Computationally and Memory-Efficient Robust Pr...   \n",
       "3           MAPL: Model Agnostic Peer-to-peer Learning   \n",
       "4    Gegenbauer Graph Neural Networks for Time-vary...   \n",
       "..                                                 ...   \n",
       "981  Andes: Defining and Enhancing Quality-of-Exper...   \n",
       "982         Timely Communications for Remote Inference   \n",
       "983  An Efficient Reconstructed Differential Evolut...   \n",
       "984  Causally Inspired Regularization Enables Domai...   \n",
       "985  OmniSearchSage: Multi-Task Multi-Entity Embedd...   \n",
       "\n",
       "                                              abstract         date  \n",
       "0    Does language help make sense of the visual wo...  01-Apr-2024  \n",
       "1    Mobile apps have embraced user privacy by movi...  01-Apr-2024  \n",
       "2    In the current data-intensive era, big data ha...  01-Apr-2024  \n",
       "3    Effective collaboration among heterogeneous cl...  01-Apr-2024  \n",
       "4    Reconstructing time-varying graph signals (or ...  01-Apr-2024  \n",
       "..                                                 ...          ...  \n",
       "981  The advent of large language models (LLMs) has...   2024-04-25  \n",
       "982  In this paper, we analyze the impact of data f...   2024-04-25  \n",
       "983  Complex single-objective bounded problems are ...   2024-04-25  \n",
       "984  Given a causal graph representing the data-gen...   2024-04-25  \n",
       "985  In this paper, we present OmniSearchSage, a ve...   2024-04-25  \n",
       "\n",
       "[836 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'abstract', 'date', '__index_level_0__'],\n",
       "    num_rows: 836\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled_embeddings=[]\n",
    "unlabelled_dataset = Dataset.from_pandas(unlabelled_data)\n",
    "\n",
    "unlabelled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(unlabelled_dataset)):\n",
    "    input_string = unlabelled_dataset['title'][i] + ' ' + unlabelled_dataset['abstract'][i]\n",
    "    tokenized_input = tokenizer(input_string, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
    "    tokenized_input = {k: v.cuda() for k, v in tokenized_input.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # embedding = outputs.last_hidden_state[:,1,-1,:].squeeze(0)\n",
    "        # print(last_hidden_states.shape)\n",
    "        # print(pooled_output.shape)\n",
    "        unlabelled_embeddings.append(pooled_output.cpu())\n",
    "        # print(pooled_output)\n",
    "        # print(embedding.shape)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "(836, 1, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.567787 , 4.5405173, 5.0938754, 4.831873 , 5.1816335, 4.868515 ,\n",
       "       5.4974885, 5.9082227, 4.2132583, 5.625834 , 5.6464567, 5.361004 ,\n",
       "       3.844457 , 4.5100245, 4.0481462, 4.558012 , 4.713201 , 4.0647397,\n",
       "       5.8034663, 4.9440107, 5.4197097, 4.325507 , 6.051324 , 4.6337714,\n",
       "       4.786901 , 5.8218307, 5.487824 , 5.0715733, 4.4352064, 3.9957843,\n",
       "       5.117768 , 5.985867 , 4.779816 , 5.999028 , 7.5814557, 3.792974 ,\n",
       "       4.933277 , 4.278781 , 4.883603 , 5.008872 , 4.5388517, 4.632354 ,\n",
       "       5.421606 , 5.5741987, 6.125299 , 4.2091293, 5.305968 , 4.4546947,\n",
       "       4.94908  , 4.3075233, 3.5731041, 5.0370245, 5.049589 , 3.222282 ,\n",
       "       4.1869593, 4.7607794, 4.989899 , 3.8067276, 4.810914 , 4.068364 ,\n",
       "       3.8329496, 4.044307 , 4.72099  , 4.6360154, 3.4335656, 5.683633 ,\n",
       "       4.822538 , 6.6259565, 5.9705915, 4.8541484, 5.262266 , 5.2582293,\n",
       "       5.5405574, 5.64006  , 5.5847335, 5.984848 , 5.493729 , 4.848004 ,\n",
       "       5.064016 , 4.989391 , 5.355492 , 4.824261 , 5.604746 , 4.6043835,\n",
       "       5.573344 , 4.557548 , 5.952765 , 5.6653376, 3.8727357, 5.1841497,\n",
       "       3.5004852, 5.107303 , 4.955592 , 4.765151 , 5.647017 , 3.9322536,\n",
       "       3.5861628, 5.5720363, 5.33154  , 4.425757 , 4.7275934, 5.095644 ,\n",
       "       5.3268676, 4.816057 , 5.681362 , 4.594667 , 5.718668 , 5.602535 ,\n",
       "       4.907375 , 4.9322615, 5.2948866, 5.027954 , 5.0612545, 5.0371923,\n",
       "       5.3529463, 4.9434366, 4.221768 , 4.364512 , 4.59502  , 4.572436 ,\n",
       "       4.764614 , 5.2814426, 5.1658907, 4.6018577, 4.481025 , 5.2455745,\n",
       "       5.2111607, 4.318565 , 5.135659 , 5.2037053, 4.6472144, 5.7891445,\n",
       "       5.420978 , 5.431782 , 4.513982 , 5.515558 , 4.4940095, 3.7088144,\n",
       "       5.2584333, 6.406641 , 5.188859 , 5.990494 , 4.2208195, 4.605843 ,\n",
       "       5.075912 , 3.641837 , 5.9551134, 5.073582 , 4.7895045, 3.8759856,\n",
       "       6.3603635, 5.057587 , 4.6725154, 4.812612 , 5.0020475, 4.0264955,\n",
       "       3.4211164, 5.2550516, 5.609503 , 6.3871617, 5.0647364, 6.267615 ,\n",
       "       3.883037 , 4.53671  , 4.9485207, 4.797883 , 4.564227 , 5.0221086,\n",
       "       4.351519 , 5.44717  , 6.247247 , 6.392619 , 3.2987347, 4.722856 ,\n",
       "       4.9974465, 5.476433 , 4.385707 , 4.6030755, 3.6514144, 3.8026896,\n",
       "       5.318499 , 4.340637 , 5.00201  , 4.8409038, 4.3758516, 3.6295311,\n",
       "       5.559875 , 5.128277 , 5.124438 , 6.4153   , 4.7321677, 5.1046586,\n",
       "       4.347812 , 5.2638407, 3.9144826, 3.6922293, 5.1442204, 5.2275753,\n",
       "       5.126871 , 6.427075 , 5.0788193, 4.1071267, 5.457714 , 5.029025 ,\n",
       "       6.268619 , 5.279768 , 5.509875 , 6.3399305, 6.011679 , 4.353983 ,\n",
       "       4.465505 , 5.283575 , 5.0084066, 4.8648386, 5.3704495, 3.325347 ,\n",
       "       5.8891664, 5.1632857, 4.581535 , 4.819339 , 5.095703 , 3.5972803,\n",
       "       3.877376 , 4.983978 , 6.4124203, 4.472367 , 4.1660247, 4.713411 ,\n",
       "       5.701066 , 5.302299 , 5.0700607, 4.5463243, 3.9863775, 4.779143 ,\n",
       "       4.6641245, 5.8245525, 5.216593 , 4.1385117, 5.443125 , 4.098396 ,\n",
       "       5.276315 , 4.225118 , 5.6755347, 5.817039 , 5.079154 , 4.352224 ,\n",
       "       5.062161 , 5.617069 , 3.662997 , 5.148609 , 5.9858155, 5.112924 ,\n",
       "       3.848791 , 4.906619 , 5.124892 , 4.8861275, 5.9351077, 4.9117007,\n",
       "       3.8116913, 4.150043 , 5.607831 , 4.4875007, 6.156476 , 4.227493 ,\n",
       "       5.004509 , 5.273611 , 3.5344756, 5.675391 , 6.01858  , 5.455966 ,\n",
       "       4.080697 , 5.1514835, 5.734435 , 5.672405 , 5.044552 , 4.419731 ,\n",
       "       5.2676353, 3.8898146, 4.579073 , 5.155155 , 5.627752 , 5.4930167,\n",
       "       5.509445 , 6.0822706, 5.0401306, 5.1301737, 5.110571 , 4.6451006,\n",
       "       4.9409223, 4.905071 , 4.9010906, 5.3345056, 4.5176387, 5.572004 ,\n",
       "       4.1473885, 5.832099 , 5.6009974, 5.15971  , 5.8364096, 4.6798515,\n",
       "       4.72347  , 5.06756  , 4.978089 , 4.9098067, 5.0537205, 4.2058964,\n",
       "       5.1064363, 5.7032814, 5.6360064, 5.1015973, 5.1924553, 5.6355605,\n",
       "       5.742477 , 5.3813066, 6.4446626, 4.1550803, 4.749994 , 4.005371 ,\n",
       "       4.148427 , 5.4855967, 4.4888253, 6.6653976, 4.5531826, 5.492144 ,\n",
       "       4.619109 , 4.563519 , 5.5164948, 5.1095457, 6.3103333, 5.817218 ,\n",
       "       4.588024 , 3.9082372, 5.7940164, 4.072724 , 5.7481833, 5.0233545,\n",
       "       4.567272 , 5.2931275, 6.0140743, 5.3802595, 5.1256785, 5.375347 ,\n",
       "       4.609788 , 4.52286  , 5.1944003, 4.1077447, 5.3922505, 4.233817 ,\n",
       "       4.7118773, 6.42123  , 6.1106963, 4.7980175, 4.8367367, 5.248014 ,\n",
       "       4.4328423, 5.0903583, 6.070394 , 4.416718 , 5.390742 , 5.0073195,\n",
       "       5.489678 , 6.036275 , 6.1259365, 6.096358 , 6.0843306, 4.596813 ,\n",
       "       6.149279 , 3.5927222, 4.7981625, 4.8636456, 5.759366 , 4.9654474,\n",
       "       4.0804987, 5.343996 , 5.3010926, 4.4192967, 4.8966494, 4.628387 ,\n",
       "       5.220434 , 4.8691688, 4.6499524, 4.3904123, 4.7371902, 5.449565 ,\n",
       "       4.5494556, 5.0833693, 5.8136396, 4.9877844, 5.655547 , 4.147068 ,\n",
       "       4.8668833, 5.164754 , 3.8484201, 3.977977 , 4.4013805, 5.544152 ,\n",
       "       4.612638 , 5.1576376, 4.529657 , 4.0442147, 3.8390012, 5.2309017,\n",
       "       4.847624 , 5.766922 , 4.1727753, 5.047701 , 5.507039 , 5.0708027,\n",
       "       5.282187 , 4.5385256, 5.4437146, 5.5830774, 4.589333 , 5.1773415,\n",
       "       5.5421767, 5.1778955, 4.167227 , 5.3066483, 3.949802 , 5.792325 ,\n",
       "       5.5381203, 5.5831943, 4.9974656, 5.043068 , 5.572046 , 3.3582237,\n",
       "       3.317689 , 5.499262 , 5.0976825, 5.906695 , 4.0050855, 6.062202 ,\n",
       "       5.707227 , 5.780719 , 4.6975584, 5.1134367, 5.4551163, 4.931107 ,\n",
       "       5.3259344, 5.261263 , 4.8657975, 4.3580933, 5.5268993, 3.8058105,\n",
       "       4.8494525, 3.834542 , 5.0439587, 4.761883 , 4.7974195, 4.51129  ,\n",
       "       3.2102277, 5.1461377, 5.269987 , 5.2272677, 5.1513934, 5.511323 ,\n",
       "       4.985054 , 4.502212 , 4.7971764, 5.933238 , 5.2570577, 5.2143025,\n",
       "       5.4768567, 5.4576054, 4.7419376, 5.2139983, 4.9076476, 5.058853 ,\n",
       "       5.118854 , 3.4894342, 3.6434648, 3.8093631, 4.408042 , 5.1965876,\n",
       "       4.454429 , 5.4973464, 4.8861566, 5.1153193, 4.4095078, 5.416291 ,\n",
       "       4.6722617, 5.209274 , 5.896383 , 5.2834487, 5.933452 , 4.4076757,\n",
       "       5.008189 , 4.7024236, 5.977741 , 6.109851 , 5.151489 , 5.559801 ,\n",
       "       5.718858 , 5.1810913, 4.7528386, 3.8891108, 4.4887085, 6.0438204,\n",
       "       4.342279 , 4.6137614, 3.5619092, 5.416045 , 4.809234 , 4.6942377,\n",
       "       6.0325384, 4.266753 , 5.531011 , 4.6216555, 5.170407 , 5.3941336,\n",
       "       6.064105 , 4.7299857, 5.621873 , 5.739076 , 4.6446424, 5.4633617,\n",
       "       4.116131 , 5.4233794, 4.106995 , 4.4806685, 4.7057967, 5.2128553,\n",
       "       4.031323 , 4.4748616, 5.149122 , 4.7372723, 4.558784 , 4.8063354,\n",
       "       5.46259  , 6.3652797, 6.6097937, 5.3483057, 4.459256 , 5.607522 ,\n",
       "       5.4239607, 4.7027383, 3.041486 , 4.516066 , 4.690599 , 3.7165844,\n",
       "       5.2027764, 4.2345767, 4.738368 , 5.90425  , 4.61623  , 5.446766 ,\n",
       "       5.626508 , 5.6456623, 5.954308 , 4.108972 , 5.515273 , 4.724138 ,\n",
       "       5.077613 , 5.928393 , 3.7593708, 5.317307 , 4.3693805, 5.0339007,\n",
       "       4.8235226, 4.7847996, 4.759729 , 5.041718 , 5.9289017, 4.1015463,\n",
       "       5.563938 , 4.766667 , 5.0508075, 4.919872 , 3.6878252, 4.9113193,\n",
       "       3.7386446, 5.1837983, 5.112303 , 5.112158 , 4.642328 , 3.2284353,\n",
       "       5.603267 , 5.287281 , 4.784395 , 5.5399876, 6.050447 , 4.443708 ,\n",
       "       5.235063 , 3.9083202, 6.145017 , 4.816618 , 4.752456 , 5.1659775,\n",
       "       5.2510805, 5.048619 , 5.379571 , 5.266823 , 4.1736636, 4.657894 ,\n",
       "       4.1199284, 4.7027392, 4.9260025, 5.1460776, 4.2621045, 5.4357553,\n",
       "       5.304013 , 4.970737 , 5.1372957, 5.886666 , 5.279531 , 5.098938 ,\n",
       "       5.4628124, 5.8491387, 4.909591 , 4.5882554, 5.916652 , 5.537292 ,\n",
       "       3.5215845, 5.3727546, 4.84481  , 4.8211136, 4.1121726, 4.980862 ,\n",
       "       4.762541 , 6.095361 , 4.2989087, 5.5526824, 4.583954 , 4.2961326,\n",
       "       5.713716 , 4.853216 , 4.2438397, 5.696031 , 4.4574027, 5.0245914,\n",
       "       6.316451 , 5.484015 , 6.0368195, 5.117997 , 3.987205 , 5.809763 ,\n",
       "       5.0007315, 5.2527237, 5.3873587, 4.8870296, 3.7679548, 5.4803   ,\n",
       "       5.071937 , 4.8048162, 6.097263 , 5.4780827, 4.4206576, 4.885047 ,\n",
       "       5.8275886, 4.289811 , 5.1028104, 5.788837 , 5.415909 , 4.4476824,\n",
       "       5.1202955, 4.771027 , 4.0230937, 4.933021 , 5.4434404, 5.554702 ,\n",
       "       4.5990133, 4.3064566, 4.627681 , 4.2781386, 5.4192758, 5.4908986,\n",
       "       6.033869 , 4.704845 , 4.245189 , 4.8810306, 4.574758 , 5.5561357,\n",
       "       5.648532 , 4.7826138, 5.164899 , 4.130406 , 4.4419184, 4.6840687,\n",
       "       5.134729 , 4.263447 , 4.7997065, 5.4625177, 5.8815823, 6.1309023,\n",
       "       5.727065 , 4.39447  , 5.674536 , 5.4124255, 5.474907 , 3.6615453,\n",
       "       3.9732585, 5.129442 , 6.147064 , 4.771426 , 6.063249 , 5.5369496,\n",
       "       5.6253905, 6.046066 , 5.10056  , 3.4882278, 4.08315  , 4.647175 ,\n",
       "       5.5561934, 5.676147 , 5.514378 , 4.9777536, 5.140501 , 4.2180896,\n",
       "       5.471221 , 4.9761443, 4.7877088, 5.210552 , 4.609731 , 4.5357065,\n",
       "       5.035929 , 4.1982927, 5.7451515, 5.635544 , 4.746507 , 5.2912   ,\n",
       "       4.289622 , 6.0038123, 4.1260695, 5.1720777, 4.673367 , 5.5190415,\n",
       "       5.0744147, 5.581887 , 5.2173586, 5.8778915, 5.222979 , 5.908811 ,\n",
       "       5.6316924, 4.8786774, 4.1811614, 5.6884055, 4.337925 , 5.9159923,\n",
       "       5.9934554, 4.3407345, 4.1479025, 5.3853226, 4.2141976, 5.320382 ,\n",
       "       4.1203384, 5.283267 , 4.461839 , 5.833313 , 4.6667128, 5.9165874,\n",
       "       4.146447 , 5.1431503, 4.516459 , 5.7250876, 5.546743 , 4.0043225,\n",
       "       4.64657  , 4.4025793, 3.6294744, 5.461657 , 6.5173483, 5.0517306,\n",
       "       4.743556 , 5.669422 , 4.9941373, 5.074538 , 4.1810427, 4.725275 ,\n",
       "       5.282088 , 4.763092 , 5.6622167, 4.886938 , 5.0254   , 5.478281 ,\n",
       "       5.0431056, 4.8382316, 5.2153077, 4.593347 , 5.7638345, 5.3998184,\n",
       "       4.420926 , 4.7925754, 3.9011283, 3.4351854, 4.274604 , 3.7843945,\n",
       "       4.282469 , 5.082156 , 5.7213593, 6.6145577, 4.009109 , 5.692143 ,\n",
       "       4.9636683, 3.865088 , 5.2147403, 4.9961896, 6.5341864, 5.270944 ,\n",
       "       5.030282 , 4.733658 , 5.2626905, 5.790192 , 5.074065 , 5.474717 ,\n",
       "       4.484676 , 5.562076 , 4.367569 , 5.608308 , 5.526749 , 5.0189705,\n",
       "       5.2816434, 3.7634866, 5.085422 , 5.675485 , 4.627026 , 4.9811745,\n",
       "       4.710048 , 5.74288  , 4.238288 , 5.1431036, 4.982006 , 5.061348 ,\n",
       "       5.069683 , 4.9415483, 4.533126 , 5.4348006, 4.838654 , 4.3119736,\n",
       "       3.5506601, 5.4611588, 5.208105 , 4.7161183, 5.292716 , 6.096438 ,\n",
       "       3.9516058, 5.0048323], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute euclidean distance between mean_embeddings and unlabelled_embeddings\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "\n",
    "mean_embeddings = mean_embeddings.cpu().numpy()\n",
    "stacked_unlabelled_embeddings = torch.stack(unlabelled_embeddings)\n",
    "stacked_unlabelled_embeddings = stacked_unlabelled_embeddings.cpu().numpy()\n",
    "print(mean_embeddings.shape)\n",
    "print(stacked_unlabelled_embeddings.shape)\n",
    "# mean_embeddings = mean_embeddings.squeeze(0)\n",
    "stacked_unlabelled_embeddings = stacked_unlabelled_embeddings.squeeze(1)\n",
    "distances = euclidean_distances(mean_embeddings.reshape(1,-1), stacked_unlabelled_embeddings)\n",
    "distances = distances.reshape(-1)\n",
    "distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of distances\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(distances, bins=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute cosine similarity between mean_embeddings and unlabelled_embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(mean_embeddings.reshape(1,-1), stacked_unlabelled_embeddings)\n",
    "cosine_similarities = cosine_similarities.reshape(-1)\n",
    "cosine_similarities\n",
    "\n",
    "# histogram of cosine similarities\n",
    "plt.hist(cosine_similarities, bins=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a neural network model to approximate g(x)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        return x\n",
    "# this model gives probability of the input being a labelled sample\n",
    "# g_x_model = NeuralNetwork()\n",
    "# g_x_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get indexes of all samples for 1st April 2024\n",
    "# indexes = []\n",
    "# for i in range(len(unlabelled_data)):\n",
    "#     if unlabelled_data['date'].iloc[i] == '01-Apr-2024':\n",
    "#         indexes.append(i)\n",
    "# indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_april_embeddings = stacked_unlabelled_embeddings[indexes]\n",
    "\n",
    "# # combine embeddings and first_april_embeddings into one dataset\n",
    "# combined_embeddings = np.concatenate((stacked_embeddings.squeeze(1).cpu(), first_april_embeddings), axis=0)\n",
    "# combined_embeddings.shape\n",
    "\n",
    "# # create labels for the combined embeddings\n",
    "# labels = np.zeros(combined_embeddings.shape[0])\n",
    "# labels[:len(embeddings)] = 1\n",
    "# len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# combined_dataset = TensorDataset(torch.tensor(combined_embeddings), torch.tensor(labels))\n",
    "# combined_dataloader = DataLoader(combined_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # train the model\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(g_x_model.parameters(), lr=0.001)\n",
    "# # model = NeuralNetwork()\n",
    "# # model.cuda()\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_loss = 0\n",
    "#     for i, data in enumerate(combined_dataloader):\n",
    "#         inputs, labels = data\n",
    "#         inputs = inputs.cuda()\n",
    "#         labels = labels.float().cuda()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = g_x_model(inputs)\n",
    "#         # print(labels)\n",
    "#         # print(outputs)\n",
    "#         loss = criterion(outputs, labels.unsqueeze(1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "#     print(f'Epoch: {epoch}, Loss: {epoch_loss}')\n",
    "#         # print(f'Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # have g(x) estimator trained\n",
    "\n",
    "# # get 1-40 and last 30 samples from the combined embeddings\n",
    "# # combined_embeddings.shape\n",
    "# # combined_embeddings[0:40].shape\n",
    "# # combined_embeddings[-30:].shape\n",
    "\n",
    "# validation_set = np.concatenate((combined_embeddings[0:len(stacked_embeddings)], combined_embeddings[-60:]), axis=0)\n",
    "# validation_labels = np.zeros(validation_set.shape[0])\n",
    "# validation_labels[:100] = 1\n",
    "# print(validation_set.shape)\n",
    "# print(validation_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set=validation_set[0:len(stacked_embeddings)].shape\n",
    "# set_tensor = torch.tensor(validation_set[0:len(stacked_embeddings)])\n",
    "# set_tensor = set_tensor.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # validation_set is the V here , P is the first 40 samples\n",
    "\n",
    "# const_prob = g_x_model(set_tensor)\n",
    "# const_prob = const_prob.cpu().detach().numpy()\n",
    "# const_prob = const_prob.reshape(-1)\n",
    "# const_prob = const_prob.mean()\n",
    "# const_prob #c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_x_samples = []\n",
    "# for i in range(len(first_april_embeddings)):\n",
    "#     p_s_x = g_x_model(torch.tensor(first_april_embeddings[i]).cuda())\n",
    "#     p_s_x = p_s_x.cpu().detach().numpy()\n",
    "#     # print(p_s_x)    \n",
    "#     # print(p_s_x)\n",
    "    \n",
    "#     print(1-const_prob,const_prob,p_s_x,1-p_s_x)\n",
    "#     w_x = ((1-const_prob)/const_prob)*(p_s_x[0]/(1-p_s_x[0]))\n",
    "#     # print(w_x)\n",
    "#     w_x_samples.append(w_x)\n",
    "# w_x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_x_samples = np.array(w_x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_x_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(w_x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model that will now predict the labels\n",
    "class NewModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_model = NewModel()\n",
    "# actual_model.cuda()\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(actual_model.parameters(), lr=0.01)\n",
    "# num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_april_embeddings = stacked_unlabelled_embeddings[indexes]\n",
    "# first_april_positive = first_april_embeddings\n",
    "# first_april_negative = first_april_embeddings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_april_positive.shape, first_april_negative.shape,stacked_embeddings.squeeze(1).cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_training_set = np.concatenate((stacked_embeddings.squeeze(1).cpu(),first_april_positive, first_april_negative), axis=0)\n",
    "# complete_training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_training_labels = np.zeros(complete_training_set.shape[0])\n",
    "# complete_training_labels[:len(stacked_embeddings)] = 1\n",
    "# complete_training_labels[len(stacked_embeddings):(len(stacked_embeddings)+first_april_positive.shape[0])] = 1\n",
    "# complete_training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_training_dataset = TensorDataset(torch.tensor(complete_training_set), torch.tensor(complete_training_labels))\n",
    "# complete_training_dataloader = DataLoader(complete_training_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_loss = 0\n",
    "#     for i, data in enumerate(complete_training_dataloader):\n",
    "#         inputs, labels = data\n",
    "#         inputs = inputs.cuda()\n",
    "#         labels = labels.float().cuda()\n",
    "#         # inputs = \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = actual_model(inputs)\n",
    "#         # print(labels)\n",
    "#         # print(outputs)\n",
    "#         # print(i)\n",
    "#         if i < len(stacked_embeddings):\n",
    "#             loss = criterion(outputs, labels.unsqueeze(1))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#         elif len(stacked_embeddings)< i < len(stacked_embeddings) + first_april_positive.shape[0]:\n",
    "#             loss = criterion(outputs, labels.unsqueeze(1))*w_x_samples[i-len(stacked_embeddings)]\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#         else:\n",
    "#             loss = criterion(outputs, labels.unsqueeze(1))*(1-w_x_samples[i-len(stacked_embeddings)-first_april_positive.shape[0]])\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#         # loss = criterion(outputs, labels.unsqueeze(1))* w_x_samples[i]\n",
    "#     print(f'Epoch: {epoch}, Loss: {epoch_loss}')\n",
    "#     # print(f'Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict on unlabelled data\n",
    "# predicted_labels = []\n",
    "# for i in range(len(first_april_embeddings)):\n",
    "#     p_s_x = actual_model(torch.tensor(first_april_embeddings[i]).cuda())\n",
    "#     p_s_x = p_s_x.cpu().detach().numpy()\n",
    "#     predicted_labels.append(p_s_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels\n",
    "# # print values >0.5 as 1 and <0.5 as 0\n",
    "# predicted_labels = np.array(predicted_labels)\n",
    "# predicted_labels = predicted_labels>0.5\n",
    "# predicted_labels = predicted_labels.astype(int)\n",
    "# predicted_labels\n",
    "\n",
    "\n",
    "# # print value counts of the predicted labels\n",
    "# unique, counts = np.unique(predicted_labels, return_counts=True)\n",
    "# dict(zip(unique, counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CrossEntropyLoss class \n",
    "class CrossEntropyLossCustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLossCustom, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, y, w_x):\n",
    "        x = self.softmax(x)\n",
    "        loss = -torch.mean(w_x*(y*torch.log(x)))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(date):\n",
    "    g_x_model = NeuralNetwork()\n",
    "    g_x_model.cuda()\n",
    "    indexes = []\n",
    "    for i in range(len(unlabelled_data)):\n",
    "        # if unlabelled_data['date'].iloc[i] == date:\n",
    "        indexes.append(i)\n",
    "    first_april_embeddings = stacked_unlabelled_embeddings[indexes]\n",
    "    print(first_april_embeddings.shape,\"fe shape\")\n",
    "    combined_embeddings = np.concatenate((stacked_embeddings.squeeze(1).cpu(), first_april_embeddings), axis=0)\n",
    "    labels = np.zeros(combined_embeddings.shape[0])\n",
    "    labels[:len(embeddings)] = 1\n",
    "    combined_dataset = TensorDataset(torch.tensor(combined_embeddings), torch.tensor(labels))\n",
    "    combined_dataloader = DataLoader(combined_dataset, batch_size=1, shuffle=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(g_x_model.parameters(), lr=0.00001)\n",
    "    # print(combined_embeddings,\"gesgwsg\")\n",
    "    num_epochs = 75\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, data in enumerate(combined_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.long().cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = g_x_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch: {epoch}, Loss: {epoch_loss}')\n",
    "    const_prob = torch.softmax(g_x_model(torch.tensor(combined_embeddings[0:(len(stacked_embeddings)-50)]).cuda()),dim=1)\n",
    "    # print(const_prob[0])\n",
    "    # get all values at the 1st index of the tensor\n",
    "    const_prob = const_prob[:,1]\n",
    "    print(const_prob.shape,\"lgyog\")\n",
    "    # print(const_prob.shape,\"lgyog\")\n",
    "    const_prob = const_prob.cpu().detach().numpy()\n",
    "    const_prob = const_prob.reshape(-1)\n",
    "    const_prob = const_prob.mean()\n",
    "    print(const_prob,\"vcdsg\")\n",
    "    w_x_samples = []\n",
    "\n",
    "    for i in range(len(first_april_embeddings)):\n",
    "        p_s_x = g_x_model(torch.tensor(first_april_embeddings[i]).cuda())\n",
    "        probs = torch.softmax(p_s_x, dim=0)\n",
    "        probs = probs.cpu().detach().numpy()\n",
    "        # print(probs,\"sgsg\")\n",
    "\n",
    "        # p_s_x = p_s_x.cpu().detach().numpy()\n",
    "        # break\n",
    "        w_x = ((1-const_prob)/const_prob)*(probs[1]/(1-probs[1]))\n",
    "        w_x_samples.append(w_x)\n",
    "    w_x_samples = np.array(w_x_samples)\n",
    "    # print(\"wgsg\",w_x_samples)\n",
    "    # normalize w_x_samples\n",
    "    w_x_samples = w_x_samples/w_x_samples.mean()\n",
    "    actual_model = NewModel()\n",
    "    actual_model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    actual_optimizer = optim.Adam(actual_model.parameters(), lr=0.001)\n",
    "    num_epochs = 50\n",
    "    first_april_positive = first_april_embeddings\n",
    "    first_april_negative = first_april_embeddings\n",
    "    complete_training_set = np.concatenate((stacked_embeddings.squeeze(1).cpu(),first_april_positive, first_april_negative), axis=0)\n",
    "    complete_training_labels = np.zeros(complete_training_set.shape[0])\n",
    "    complete_training_labels[:len(stacked_embeddings)] = 1\n",
    "    complete_training_labels[len(stacked_embeddings):(len(stacked_embeddings)+first_april_positive.shape[0])] = 1\n",
    "    complete_training_dataset = TensorDataset(torch.tensor(complete_training_set), torch.tensor(complete_training_labels))\n",
    "\n",
    "    complete_training_dataloader = DataLoader(complete_training_dataset, batch_size=1, shuffle=False)\n",
    "    return complete_training_dataloader, w_x_samples, first_april_embeddings,first_april_positive,first_april_negative,stacked_embeddings\n",
    "\n",
    "def predict(complete_training_dataloader, w_x_samples, first_april_embeddings,first_april_positive,first_april_negative,stacked_embeddings):\n",
    "    actual_model = NewModel()\n",
    "    actual_model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    actual_optimizer = optim.Adam(actual_model.parameters(), lr=0.00001)\n",
    "    num_epochs = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, data in enumerate(complete_training_dataloader):\n",
    "            \n",
    "            wt= torch.tensor(0)\n",
    "            wt = wt.cuda()\n",
    "            if i < len(stacked_embeddings):\n",
    "                wt = 1\n",
    "            elif len(stacked_embeddings)< i < len(stacked_embeddings) + first_april_positive.shape[0]:\n",
    "                wt=w_x_samples[i-len(stacked_embeddings)]\n",
    "            else:\n",
    "                wt=(1-w_x_samples[i-len(stacked_embeddings)-first_april_positive.shape[0]])\n",
    "            # print(wt,i)\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.long().cuda()\n",
    "            actual_optimizer.zero_grad()\n",
    "            outputs = actual_model(inputs)\n",
    "            # print(outputs,labels)\n",
    "            loss_actual = criterion(outputs, labels)*wt\n",
    "            # print(loss_actual)\n",
    "            loss_actual.backward()\n",
    "            actual_optimizer.step()\n",
    "            epoch_loss += loss_actual.item()\n",
    "            \n",
    "        print(f'Epoch: {epoch}, Loss: {epoch_loss/len(complete_training_dataloader)}')\n",
    "    predicted_labels = []\n",
    "    for i in range(len(first_april_embeddings)):\n",
    "        p_s_x = actual_model(torch.tensor(first_april_embeddings[i]).cuda())\n",
    "        # print(p_s_x)\n",
    "        p_s_x = p_s_x.cpu().detach().numpy()\n",
    "        predicted_labels.append(p_s_x)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    print(\"predicted labels\",predicted_labels)\n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836, 768) fe shape\n",
      "Epoch: 0, Loss: 388.5160922091454\n",
      "Epoch: 1, Loss: 467.741330396384\n",
      "Epoch: 2, Loss: 380.73664939776063\n",
      "Epoch: 3, Loss: 330.1784589905292\n",
      "Epoch: 4, Loss: 308.1205305522308\n",
      "Epoch: 5, Loss: 273.5168316140771\n",
      "Epoch: 6, Loss: 260.82890295097604\n",
      "Epoch: 7, Loss: 240.51057925145142\n",
      "Epoch: 8, Loss: 229.17780393362045\n",
      "Epoch: 9, Loss: 220.40594868501648\n",
      "Epoch: 10, Loss: 212.6123870149022\n",
      "Epoch: 11, Loss: 206.7710744033102\n",
      "Epoch: 12, Loss: 201.9012194161769\n",
      "Epoch: 13, Loss: 195.45873518136796\n",
      "Epoch: 14, Loss: 191.40295459510526\n",
      "Epoch: 15, Loss: 187.8171668588766\n",
      "Epoch: 16, Loss: 184.3489509546198\n",
      "Epoch: 17, Loss: 181.57823231915245\n",
      "Epoch: 18, Loss: 178.7521238301415\n",
      "Epoch: 19, Loss: 176.48344383839867\n",
      "Epoch: 20, Loss: 174.30780076954397\n",
      "Epoch: 21, Loss: 172.40006967895897\n",
      "Epoch: 22, Loss: 170.38379110442474\n",
      "Epoch: 23, Loss: 168.6821566511353\n",
      "Epoch: 24, Loss: 167.00510476375348\n",
      "Epoch: 25, Loss: 165.32572316036385\n",
      "Epoch: 26, Loss: 163.62038918136386\n",
      "Epoch: 27, Loss: 162.04541547079862\n",
      "Epoch: 28, Loss: 160.64976766855398\n",
      "Epoch: 29, Loss: 159.3207724417589\n",
      "Epoch: 30, Loss: 158.06562665147794\n",
      "Epoch: 31, Loss: 156.76969290775742\n",
      "Epoch: 32, Loss: 155.68335786750686\n",
      "Epoch: 33, Loss: 154.5536556468869\n",
      "Epoch: 34, Loss: 153.55554289337306\n",
      "Epoch: 35, Loss: 152.37344521114574\n",
      "Epoch: 36, Loss: 151.36668913898757\n",
      "Epoch: 37, Loss: 150.29695814585284\n",
      "Epoch: 38, Loss: 149.34939064357604\n",
      "Epoch: 39, Loss: 148.2317804085942\n",
      "Epoch: 40, Loss: 147.0741933678728\n",
      "Epoch: 41, Loss: 146.08941705762845\n",
      "Epoch: 42, Loss: 145.16367117180562\n",
      "Epoch: 43, Loss: 144.237432683909\n",
      "Epoch: 44, Loss: 143.3552855545786\n",
      "Epoch: 45, Loss: 142.43241639361077\n",
      "Epoch: 46, Loss: 141.54831460777314\n",
      "Epoch: 47, Loss: 140.6801662749349\n",
      "Epoch: 48, Loss: 139.65291972602245\n",
      "Epoch: 49, Loss: 138.86825523978405\n",
      "Epoch: 50, Loss: 138.01319932128717\n",
      "Epoch: 51, Loss: 137.16771435930423\n",
      "Epoch: 52, Loss: 136.24335094949856\n",
      "Epoch: 53, Loss: 135.55454272996394\n",
      "Epoch: 54, Loss: 134.63346625258782\n",
      "Epoch: 55, Loss: 133.90271244501673\n",
      "Epoch: 56, Loss: 133.17709093576286\n",
      "Epoch: 57, Loss: 132.45961145080128\n",
      "Epoch: 58, Loss: 131.5702499436511\n",
      "Epoch: 59, Loss: 130.90588666057374\n",
      "Epoch: 60, Loss: 129.9687757220281\n",
      "Epoch: 61, Loss: 129.26412771042487\n",
      "Epoch: 62, Loss: 128.45017782780997\n",
      "Epoch: 63, Loss: 127.77219186638195\n",
      "Epoch: 64, Loss: 126.95900766076647\n",
      "Epoch: 65, Loss: 126.16232004788799\n",
      "Epoch: 66, Loss: 125.52777991539187\n",
      "Epoch: 67, Loss: 124.85947743241377\n",
      "Epoch: 68, Loss: 124.07467825350841\n",
      "Epoch: 69, Loss: 123.43106442968383\n",
      "Epoch: 70, Loss: 122.63493577419013\n",
      "Epoch: 71, Loss: 121.99348810684887\n",
      "Epoch: 72, Loss: 121.22705006842034\n",
      "Epoch: 73, Loss: 120.61482820319952\n",
      "Epoch: 74, Loss: 119.82796629936956\n",
      "torch.Size([263]) lgyog\n",
      "0.09167762 vcdsg\n",
      "Epoch: 0, Loss: 0.1669179424079199\n",
      "Epoch: 1, Loss: -0.02006770322515432\n",
      "Epoch: 2, Loss: -0.2510164266732382\n",
      "Epoch: 3, Loss: -0.4134462762903334\n",
      "Epoch: 4, Loss: -0.5594339537387142\n",
      "Epoch: 5, Loss: -0.7116074136819239\n",
      "Epoch: 6, Loss: -0.8846840708077341\n",
      "Epoch: 7, Loss: -1.0764776054020972\n",
      "Epoch: 8, Loss: -1.2927328064664143\n",
      "Epoch: 9, Loss: -1.5466221543638885\n",
      "Epoch: 10, Loss: -1.8297425598103314\n",
      "Epoch: 11, Loss: -2.1336367872701874\n",
      "Epoch: 12, Loss: -2.48458628653428\n",
      "Epoch: 13, Loss: -2.8625181358753937\n",
      "Epoch: 14, Loss: -3.2842752226322407\n",
      "Epoch: 15, Loss: -3.751136268093767\n",
      "Epoch: 16, Loss: -4.251719333154874\n",
      "Epoch: 17, Loss: -4.790890866973422\n",
      "Epoch: 18, Loss: -5.3694018170940305\n",
      "Epoch: 19, Loss: -5.991578249853427\n",
      "Epoch: 20, Loss: -6.65580380773359\n",
      "Epoch: 21, Loss: -7.371477594587506\n",
      "Epoch: 22, Loss: -8.127981980786766\n",
      "Epoch: 23, Loss: -8.932607067585153\n",
      "Epoch: 24, Loss: -9.797546101826596\n",
      "Epoch: 25, Loss: -10.704245723356923\n",
      "Epoch: 26, Loss: -11.67037227575675\n",
      "Epoch: 27, Loss: -12.686763993836085\n",
      "Epoch: 28, Loss: -13.762411743221714\n",
      "Epoch: 29, Loss: -14.883402121698422\n",
      "Epoch: 30, Loss: -16.066364140402886\n",
      "Epoch: 31, Loss: -17.307731246952464\n",
      "Epoch: 32, Loss: -18.61541433882948\n",
      "Epoch: 33, Loss: -19.982618287419108\n",
      "Epoch: 34, Loss: -21.405632354328482\n",
      "Epoch: 35, Loss: -22.894490121080914\n",
      "Epoch: 36, Loss: -24.456061524824648\n",
      "Epoch: 37, Loss: -26.081977365997798\n",
      "Epoch: 38, Loss: -27.769382250885737\n",
      "Epoch: 39, Loss: -29.51881588112018\n",
      "Epoch: 40, Loss: -31.34336857967568\n",
      "Epoch: 41, Loss: -33.24017546453231\n",
      "Epoch: 42, Loss: -35.210341548286614\n",
      "Epoch: 43, Loss: -37.25353553976486\n",
      "Epoch: 44, Loss: -39.36257699530842\n",
      "Epoch: 45, Loss: -41.581990048145265\n",
      "Epoch: 46, Loss: -43.83739313651286\n",
      "Epoch: 47, Loss: -46.19966689815352\n",
      "Epoch: 48, Loss: -48.624309260127994\n",
      "Epoch: 49, Loss: -51.15876776210885\n",
      "Epoch: 50, Loss: -53.750870831346155\n",
      "Epoch: 51, Loss: -56.41619011650166\n",
      "Epoch: 52, Loss: -59.211025999158295\n",
      "Epoch: 53, Loss: -62.0671810913105\n",
      "Epoch: 54, Loss: -65.0294604220926\n",
      "Epoch: 55, Loss: -68.08286995752916\n",
      "Epoch: 56, Loss: -71.24750303702345\n",
      "Epoch: 57, Loss: -74.47009095611484\n",
      "Epoch: 58, Loss: -77.8115628666274\n",
      "Epoch: 59, Loss: -81.2531747964422\n",
      "Epoch: 60, Loss: -84.78815376303876\n",
      "Epoch: 61, Loss: -88.44271902596142\n",
      "Epoch: 62, Loss: -92.19418172932009\n",
      "Epoch: 63, Loss: -96.03132220545595\n",
      "Epoch: 64, Loss: -99.98934420307448\n",
      "Epoch: 65, Loss: -104.0621827602663\n",
      "Epoch: 66, Loss: -108.23301815069276\n",
      "Epoch: 67, Loss: -112.52085255238988\n",
      "Epoch: 68, Loss: -116.90944935400553\n",
      "Epoch: 69, Loss: -121.4154327513951\n",
      "Epoch: 70, Loss: -126.01897095403436\n",
      "Epoch: 71, Loss: -130.7665893898584\n",
      "Epoch: 72, Loss: -135.6185336435493\n",
      "Epoch: 73, Loss: -140.57208460013595\n",
      "Epoch: 74, Loss: -145.63132921392702\n",
      "Epoch: 75, Loss: -150.8228282294445\n",
      "Epoch: 76, Loss: -156.12749561648485\n",
      "Epoch: 77, Loss: -161.54967378820427\n",
      "Epoch: 78, Loss: -167.10812143687434\n",
      "Epoch: 79, Loss: -172.81422875350958\n",
      "Epoch: 80, Loss: -178.64104514724067\n",
      "Epoch: 81, Loss: -184.59168167462175\n",
      "Epoch: 82, Loss: -190.67609990580706\n",
      "Epoch: 83, Loss: -196.8980126074311\n",
      "Epoch: 84, Loss: -203.2342529554916\n",
      "Epoch: 85, Loss: -209.74135019781835\n",
      "Epoch: 86, Loss: -216.29828274344277\n",
      "Epoch: 87, Loss: -223.1215323926263\n",
      "Epoch: 88, Loss: -230.01004083312148\n",
      "Epoch: 89, Loss: -237.06133511274115\n",
      "Epoch: 90, Loss: -244.18342084394527\n",
      "Epoch: 91, Loss: -251.5820270326405\n",
      "Epoch: 92, Loss: -258.9677185091295\n",
      "Epoch: 93, Loss: -266.6570055341034\n",
      "Epoch: 94, Loss: -274.3512066316357\n",
      "Epoch: 95, Loss: -282.31529486600687\n",
      "Epoch: 96, Loss: -290.3602536036323\n",
      "Epoch: 97, Loss: -298.5494300653776\n",
      "Epoch: 98, Loss: -306.95405325119503\n",
      "Epoch: 99, Loss: -315.39510768509024\n",
      "predicted labels [[-481.35657    569.00464  ]\n",
      " [-107.387924   172.86533  ]\n",
      " [ 314.37897   -262.30405  ]\n",
      " ...\n",
      " [ 295.4826    -244.23195  ]\n",
      " [-112.5979     182.08144  ]\n",
      " [   7.3455806   51.097874 ]]\n"
     ]
    }
   ],
   "source": [
    "complete_training_dataloader, w_x_samples, first_april_embeddings,first_april_positive,first_april_negative,stacked_embeddings = pipeline('12-Apr-2024')\n",
    "predicted_labels = predict(complete_training_dataloader, w_x_samples, first_april_embeddings,first_april_positive,first_april_negative,stacked_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-35.72483   ,  41.007298  ],\n",
       "       [  0.06531733,   5.063669  ],\n",
       "       [ 19.629444  , -13.999677  ],\n",
       "       ...,\n",
       "       [ 23.090683  , -17.461338  ],\n",
       "       [ -1.3720555 ,   6.6948295 ],\n",
       "       [  4.8271365 ,   0.07027562]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take argmax of the predicted labels\n",
    "actual_labels = np.argmax(predicted_labels, axis=1)\n",
    "actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 523, 1: 313}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get value counts of the actual labels\n",
    "unique, counts = np.unique(actual_labels, return_counts=True)\n",
    "dict(zip(unique, counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oai:arXiv.org:2403.19669v1</td>\n",
       "      <td>Analyzing the Roles of Language and Vision in ...</td>\n",
       "      <td>Does language help make sense of the visual wo...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oai:arXiv.org:2403.19717v1</td>\n",
       "      <td>A Picture is Worth 500 Labels: A Case Study of...</td>\n",
       "      <td>Mobile apps have embraced user privacy by movi...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oai:arXiv.org:2403.19721v1</td>\n",
       "      <td>Computationally and Memory-Efficient Robust Pr...</td>\n",
       "      <td>In the current data-intensive era, big data ha...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oai:arXiv.org:2403.19792v1</td>\n",
       "      <td>MAPL: Model Agnostic Peer-to-peer Learning</td>\n",
       "      <td>Effective collaboration among heterogeneous cl...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oai:arXiv.org:2403.19800v1</td>\n",
       "      <td>Gegenbauer Graph Neural Networks for Time-vary...</td>\n",
       "      <td>Reconstructing time-varying graph signals (or ...</td>\n",
       "      <td>01-Apr-2024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>http://arxiv.org/abs/2404.16283v1</td>\n",
       "      <td>Andes: Defining and Enhancing Quality-of-Exper...</td>\n",
       "      <td>The advent of large language models (LLMs) has...</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>http://arxiv.org/abs/2404.16281v1</td>\n",
       "      <td>Timely Communications for Remote Inference</td>\n",
       "      <td>In this paper, we analyze the impact of data f...</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>http://arxiv.org/abs/2404.16280v1</td>\n",
       "      <td>An Efficient Reconstructed Differential Evolut...</td>\n",
       "      <td>Complex single-objective bounded problems are ...</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>http://arxiv.org/abs/2404.16277v1</td>\n",
       "      <td>Causally Inspired Regularization Enables Domai...</td>\n",
       "      <td>Given a causal graph representing the data-gen...</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>http://arxiv.org/abs/2404.16260v1</td>\n",
       "      <td>OmniSearchSage: Multi-Task Multi-Entity Embedd...</td>\n",
       "      <td>In this paper, we present OmniSearchSage, a ve...</td>\n",
       "      <td>2024-04-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>836 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0           oai:arXiv.org:2403.19669v1   \n",
       "1           oai:arXiv.org:2403.19717v1   \n",
       "2           oai:arXiv.org:2403.19721v1   \n",
       "3           oai:arXiv.org:2403.19792v1   \n",
       "4           oai:arXiv.org:2403.19800v1   \n",
       "..                                 ...   \n",
       "981  http://arxiv.org/abs/2404.16283v1   \n",
       "982  http://arxiv.org/abs/2404.16281v1   \n",
       "983  http://arxiv.org/abs/2404.16280v1   \n",
       "984  http://arxiv.org/abs/2404.16277v1   \n",
       "985  http://arxiv.org/abs/2404.16260v1   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Analyzing the Roles of Language and Vision in ...   \n",
       "1    A Picture is Worth 500 Labels: A Case Study of...   \n",
       "2    Computationally and Memory-Efficient Robust Pr...   \n",
       "3           MAPL: Model Agnostic Peer-to-peer Learning   \n",
       "4    Gegenbauer Graph Neural Networks for Time-vary...   \n",
       "..                                                 ...   \n",
       "981  Andes: Defining and Enhancing Quality-of-Exper...   \n",
       "982         Timely Communications for Remote Inference   \n",
       "983  An Efficient Reconstructed Differential Evolut...   \n",
       "984  Causally Inspired Regularization Enables Domai...   \n",
       "985  OmniSearchSage: Multi-Task Multi-Entity Embedd...   \n",
       "\n",
       "                                              abstract         date  \\\n",
       "0    Does language help make sense of the visual wo...  01-Apr-2024   \n",
       "1    Mobile apps have embraced user privacy by movi...  01-Apr-2024   \n",
       "2    In the current data-intensive era, big data ha...  01-Apr-2024   \n",
       "3    Effective collaboration among heterogeneous cl...  01-Apr-2024   \n",
       "4    Reconstructing time-varying graph signals (or ...  01-Apr-2024   \n",
       "..                                                 ...          ...   \n",
       "981  The advent of large language models (LLMs) has...   2024-04-25   \n",
       "982  In this paper, we analyze the impact of data f...   2024-04-25   \n",
       "983  Complex single-objective bounded problems are ...   2024-04-25   \n",
       "984  Given a causal graph representing the data-gen...   2024-04-25   \n",
       "985  In this paper, we present OmniSearchSage, a ve...   2024-04-25   \n",
       "\n",
       "     predicted_labels  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "..                ...  \n",
       "981                 0  \n",
       "982                 0  \n",
       "983                 0  \n",
       "984                 1  \n",
       "985                 0  \n",
       "\n",
       "[836 rows x 5 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append actual labels to the unlabelled data\n",
    "unlabelled_data['predicted_labels'] = actual_labels\n",
    "unlabelled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlabelled_data.to_csv('saved_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
